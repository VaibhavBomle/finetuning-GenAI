# -*- coding: utf-8 -*-
"""Finetuning_of_Zephyr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBZqjzeNzKJLYPPbs4ygFNm5oiMZAt--

**Install required library**

Here we are train model with respect to chatbot
"""

!pip install accelerate

!pip install peft

!pip install bitsandbytes

!pip install trl py7zr auto-gptq optimum

!pip install git+https://github.com/huggingface/transformers

!pip install trl

!pip install dataset

from huggingface_hub import notebook_login
notebook_login()

import torch
from datasets import load_dataset,Dataset
from peft import LoraConfig,prepare_model_for_kbit_training,get_peft_model
from transformers import AutoModelForCausalLM,AutoTokenizer,GPTQConfig,TrainingArguments
from trl import SFTTrainer

class Config:
  MODEL_ID = "TheBloke/zephyr-7B-alpha-GPTQ"
  DATASET_ID = "bitext/Bitext-customer-support-llm-chatbot-training-dataset"
  CONTEXT_FIELD = ""
  INSTRUCTION_FIELD = "instruction"
  TARGET_FIELD = "response"
  BITS = 4
  DISABLE_EXLLAMA = True
  DEVICE_MAP = "auto"
  USE_CACHE = False
  LORA_R = 16
  LORA_ALPHA = 16
  LORA_DROPOUT = 0.05
  BIAS = "none"
  TARGET_MODULES = ["q_proj", "v_proj"]
  TASK_TYPE = "CAUSAL_LM"
  OUTPUT_DIR = "zephyr-support-chatbot"
  BATCH_SIZE = 8
  GRAD_ACCUMULATION_STEPS = 1
  OPTIMIZER = "paged_adamw_32bit"
  LR = 2e-4
  LR_SCHEDULER = "cosine"
  LOGGING_STEPS = 50
  SAVE_STRATEGY = "epoch"
  NUM_TRAIN_EPOCHS = 1
  MAX_STEPS = 250
  FP16 = True
  PUSH_TO_HUB = True
  DATASET_TEXT_FIELD = "text"
  MAX_SEQ_LENGTH = 512
  PACKING = False

from os.path import split
class ZephyrTrainer:

  def __init__(self):
    # LOADING CONFIG
    self.config = Config()
    self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_ID)
    self.tokenizer.pad_token = self.tokenizer.eos_token

  def process_data_sample(self,example):
      processed_example = "<|system|>\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n<|user|>\n" + example[self.config.INSTRUCTION_FIELD] + "\n<|assistant|>\n" + example[self.config.TARGET_FIELD]
      return processed_example

  def create_dataset(self):
      #LOADING DATA
      data = load_dataset(self.config.DATASET_ID, split="train")

      print("\n=========================")
      print("DOWNLOAD DATASET")
      print("\n=========================")

      df = data.to_pandas()
      df[self.config.DATASET_TEXT_FIELD] = df[[self.config.INSTRUCTION_FIELD,self.config.TARGET_FIELD]].apply(lambda x: self.process_data_sample(x),axis=1)

      print("\n=========================")
      print("PROCESSED  DATASET")
      print(df.iloc[0])
      print("\n=========================")
      processed_data = Dataset.from_pandas(df[[self.config.DATASET_TEXT_FIELD]])


      return processed_data

  def prepare_model(self):
      # Preparing a Quantized Model below
      # GPT Config
      bnb_config = GPTQConfig(
          bits=self.config.BITS,
          disable_exllama=self.config.DISABLE_EXLLAMA,
          tokenizer=self.tokenizer
      )

      # Download Model
      model= AutoModelForCausalLM.from_pretrained(
          self.config.MODEL_ID,
          quantization_config=bnb_config,
          device_map=self.config.DEVICE_MAP
      )

      print("\n===============================")
      print("\t\t\t DOWNLOAD MODEL")
      print(model)
      print("\n===============================")

      # Model config
      model.config.use_cache=self.config.USE_CACHE
      model.config.pretraining_tp=1
      model.gradient_checkpointing_enable()
      model = prepare_model_for_kbit_training(model)

      print("\n===============================")
      print("\t\t\tMODEL CONFIG UPDATED")
      print("\n===============================")

      # Low Rank Adaptation
      peft_config = LoraConfig(
          r=self.config.LORA_R,
          lora_alpha=self.config.LORA_ALPHA,
          lora_dropout=self.config.LORA_DROPOUT,
          bias=self.config.BIAS,
          task_type=self.config.TASK_TYPE,
          target_modules=self.config.TARGET_MODULES
      )

      model = get_peft_model(model,peft_config)
      print("\n====================================================================\n")
      print("\t\t\tPREPARED MODEL FOR FINETUNING")
      print("\n====================================================================\n")
      print(model)

      return model,peft_config

  def set_training_arguments(self):
      training_arguments = TrainingArguments(
                                                output_dir=self.config.OUTPUT_DIR,
                                                per_device_train_batch_size=self.config.BATCH_SIZE,
                                                gradient_accumulation_steps=self.config.GRAD_ACCUMULATION_STEPS,
                                                optim=self.config.OPTIMIZER,
                                                learning_rate=self.config.LR,
                                                lr_scheduler_type=self.config.LR_SCHEDULER,
                                                save_strategy=self.config.SAVE_STRATEGY,
                                                logging_steps=self.config.LOGGING_STEPS,
                                                num_train_epochs=self.config.NUM_TRAIN_EPOCHS,
                                                max_steps=self.config.MAX_STEPS,
                                                fp16=self.config.FP16,
                                                push_to_hub=self.config.PUSH_TO_HUB
                                            )
      return training_arguments

  def train(self):
      data = self.create_dataset()
      model,peft_config = self.prepare_model()
      training_args = self.set_training_arguments()

      trainer = SFTTrainer(
          model=model,
          train_dataset=data,
          peft_config=peft_config,
          dataset_text_field = self.config.DATASET_TEXT_FIELD,
          args=training_args,
          tokenizer = self.tokenizer,
          packing=self.config.PACKING,
          max_seq_length=self.config.MAX_SEQ_LENGTH
      )

      trainer.train()
      print("\n==================")
      print("FINETUNING COMPLETED")
      print("\n==================")

      trainer.push_to_hub()

# Created a Object of ZephyrTrainer class then call train() method for finetuning
zephyrTrainer = ZephyrTrainer()

zephyrTrainer.train()

from peft import AutoPeftModelForCausalLM
from transformers import GenerationConfig
from transformers import AutoTokenizer
import torch

def process_data_sample(example):
  processed_example = "<|system|>\n You are a support chatbot who helps with user queries chatbot who always responds in the style of a professional.\n<|user|>\n" + example["instruction"] + "\n<|assistant|>\n"
  return processed_example

tokenizer = AutoTokenizer.from_pretrained("/content/zephyr-support-chatbot")

inp_str = process_data_sample(
    {
        "instruction":"I have a question about cancelling order {{Order Number}}"
    }
)

inputs = tokenizer(inp_str,return_tensors="pt").to("cuda")

model = AutoPeftModelForCausalLM.from_pretrained(
    "/content/zephyr-support-chatbot",
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map="cuda"
)

generation_config = GenerationConfig(
    do_sample=True,
    top_k=1,
    temperature=0.1,
    max_new_tokens=256,
    pad_token_id=tokenizer.eos_token_id
)

import time
st_time = time.time()
outputs = model.generate(**inputs, generation_config=generation_config)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print(time.time()-st_time)